#+TITLE:       Deep Learning Intro
#+AUTHOR:      Arturo Vivas
#+EMAIL:       arturo.vivas@outlook.de
#+DATE:        2016-11-25 Fr
#+URI:         /blog/2016/11/26/deep-learning-intro
#+KEYWORDS:    deep learning, machine learning, python
#+TAGS:        deep learning book
#+LANGUAGE:    en
#+OPTIONS:     H:3 num:nil toc:nil \n:nil ::t |:t ^:nil -:nil f:t *:t <:t
#+DESCRIPTION: Intro


The game for the following days is to read the brand new book Deep Leaning from Ian Goodfellow, Yoshua Bengio and Aaraon Courville. The book is at this time just aviable in digital on the [[http://www.deeplearningbook.org/]] page. The book can be pre-ordered on Amazon, where you can also find the first review of the book, from a guy named Elon Musk (Who? :-P). 

My approach will be to read most of the chapter (According to my shedule in one week :D) I will be doing a resume of the book according to what I find most relevant (Not so an easy task since the whole book is full of Information not just data, if you know what I mean). Parallel I will be testing the different concepts using mainly Tensor Flow. Legen wir mal los! 


** Deep Learning

The quintessential example of a deep learning model is the feedforward deep network or *multilayer perceptron* (MLP). A multilayer perceptron is just a mathematical function mapping some set of input values to output values. One perspective of deep learning is the idea of *learning the righ representation of the data*. Another perspective is that depth allows the computer to learn a multistep computer program.

Sequential instructions offer great power because later instructions can refer back to the results of earlier instructions. Not all of the information in a layer's activations necessarily encodes factors of variation that explain the input. The representation also stores state information that helps to execute a program that can make sense of the input.

There are two main ways of measuring the depth of a model. 
1) Based on the number of sequential instructions that must be executed to evaluate the architecture. The length will depend on the computer language used, it will also depend on which fuctions we allow to be used as individual steps in a flowchart. The measure of the computational graph.
2)  The second approach doesn't take into account the depth of the computational graph, but the depth of the graph describing how concepts are related to each other. What is known as the measure of the concept graph or the probabilistic modeling graph. 

Deep learning can be regarded as the study of models that either involve a greater amount of composition of learned functions or learned concepts than traditional machine learning does. 

Deep learning is a particular kind of machine learning that archives great power and flexibility by learning to represent the world as nested hierarchy of concepts, with each concept defined in relation to simpler concepts and more abstract representations computed in terms of less abstract ones. 

The earliest predecessors of modern deep learning were simple linear models motivated from a neuroscientigic perspective. This models were designed to take a set of $n$ input values $x_1,...,x_n$ and associate them with an output $y$. This models would learn a set of weights $w_1,...,w_n$  and compute their output $f(x,w) = x_1w_1 + ... + x_nw_n$. 

** Historical aspects of Deep Learning

There have been three waves of development of Deep Learning:

1) Deep learning known as Cybernetics in the 1940s-1960s
- The early model used during this wave was the McCulloch-Pitts Neuron (McCulloch and Pits, 1943). This linear model could recognize two different categories of inputs by testing whether $f(\textbf{x},\textbf{w})$ was positive or negative. *The weights were set by a human operator*. 
- The perceptron (Rosenblatt, 1958, 1962) in the 1950s was the *first model able to learn the weights from data*. This model was also a binary classifier. 
- The adaptative linear element (ADALINE) (Widrow and Hoff, 1960) model, differed to the perceptron mainly on the learning phase. The algorithm used to update the weights was the *stochastic gradiant desent*. 

2) Deep learning known as Connectionism in the 1980s-1990s
- This movement was called connectionism or parallel distributed processing (Rumelhart /et  al./, 1986c; McClelland /et al./, 1995). The driving idea during this century was that a large number of simple computational units can archive intelligent behavior when networked together. This concept and others from this decade are still central to today's deep learning.
- *Distributed representation* (Hinton /et al./, 1986) is also another of these concepts. The ideas is that each feature should be involved in the representation of many possible inputs.
- The sucessful use of the back-propagration algorithm (Rumelhart /et al./, 1986a; LeCun, 1987) to train deep networks was another major accomplishment. 
- In the 1990s many important advances in modeling sequences with neural networks were made. The identification of some of the fundamental mathematical difficulties in modeling long sequences (Hochreiter, 1991; Bengio /et al./, 1994). The introduction of the long short-term memory LSTM networks (Hochreiter and Schmidhuber, 1997).

3) The current resurgence under the name deep learning beginning in 2006.
- In 2006 there was a breakthrough leaded by Geoffrey Hinton. It was shown that a kind of neural network called a deep belief network could be efficiently trained using a strategy called greedy layer-wise pre-training(Hinton et al. 2006). Other followed showing that the same strateg could be used to train many other kinds of deep networks (Bengio /et al./, 2007; Ranzato et al., 2007a). This wave of neural network research popularized the use of the term *deep learning* to emphasize that researchers were now able to train deeper neural networks than had been possible before, and to focus the attention on the theoretical importance of depth (Bengio and LeCun, 2007; Delalleau and Bengio, 2001; Pascanu /et al./, 2014a; Montufar /et al./, 2014).

** About the increasing Dataset and Model SIzes

- The learning algorithms reaching human performance on complex tasks today are nearly identical to the learning algorithms that struggled to solve toy problems in the 1980s. The most important new development is that today we can provide these algorithms with the resources they need to succeed, among them an incrisingly amount of data.  
- As of 2016, a rough rule of thumb is that supervised deep learning algorithm will generally achieve acceptable performance with around 5,000 labeled examples per category, and will match or exceed human performance when trained with a dataset containing at least 10 million labeled examples. 
- Another key reaosn is that we have the computational resources to run much larger models today. In terms of the total number of neurons, neural networks have been astonishingly small until quite recently. Since the introduction of hidden units, artificial neural networks have doubled in size roughly evey 2.4 years. At the current date, the number of neurons in some models is a bit below 10,0000,000. 

** Some other interesting things from the chapter

Libreries for Deep Learning implementation:

- [[http://deeplearning.net/software/theano/][Theano]]
- [[http://deeplearning.net/software/pylearn2/][Pylearn2]]
- [[http://torch.ch/whoweare.html][Torch]]
- [[http://caffe.berkeleyvision.org/][Caffe]]
- [[http://mxnet.io/][MXNet]]
- [[https://www.tensorflow.org/][TensorFlow]]


